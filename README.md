# Data-Design-Representation
These are two individual web-scrapping projects I did for DDR course in UCD MSBA program

**Requirements for individual project 1:**
There are two parts in the first Individual project.
In the first part. I was asked to grap the data for the sold amazon gift card in ebay and find out the rate of cards sold at the price less than its value.
and here are the detailed requirements:

a) use the URL identified above and write code that loads eBay's search result page containing sold "amazon gift card". Save the result to file. Give the file the filename "amazon_gift_card_01.htm".

b) take your code in (a) and write a loop that will download the first 10 pages of search results. Save each of these pages to "amazon_gift_card_XX.htm" (XX = page number). IMPORTANT: each page request needs to be followed by a 10 second pause.  Please remember, you want your program to mimic your behavior as a human and help you make good purchasing decisions.

c) write code that loops through the pages you downloaded in (b), opens and parses them to a Python or Java xxxxsoup-object.

d) using your code in (c) and your answer to 1 (g), identify and print to screen the title, price, and shipping price of each item.

e) using RegEx, identify and print to screen gift cards that sold above face value. e., use RegEx to extract the value of a gift card from its title when possible (doesn’t need to work on all titles, > 90% success rate if sufficient). Next compare a gift card’s value to its price + shipping (free shipping should be treated as 0).  If value < price + shipping, then a gift card sells above face value.

f) What fraction of Amazon gift cards sells above face value? Why do you think this is the case?

 

In the second part, I was asked to use "BeautifulSoup" to place an order in a sport gambling website and the detailed requirements are following:

a) Following the steps we discussed in class and write code that automatically logs into the website fctables.comLinks to an external site..

b) Verify that you have successfully logged in:  use the cookies you received during log in and write code to access https://www.fctables.com/tipster/my_bets/Links to an external site..  Check whether the word “Wolfsburg” appears on the page.  Don’t look for your username to confirm that you are logged in (it won’t work) and use this page’s content instead.


**Requirements for individual project 2:**
Selenium:  The Bored Ape Yacht Club

According to Wikipedia, “[The] Bored Ape Yacht Club […] is a non-fungible token (NFT) collection built on the Ethereum blockchain.  The collection features profile pictures of cartoon apes that are procedurally generated by an algorithm.  […]  As of 2022, [Bored Ape Yacht Club’s parent company,] Yuga Labs, is valued at US$4 billion.  This is due in large part to the sales of the Bored Ape Yacht Club NFT collection totaling over US$1 billion.  Various celebrities have purchased these non-fungible tokens, including Justin Bieber, Snoop Dogg, Gwyneth Paltrow and others.”

(1)  (No programming yet,) go to https://opensea.io/collection/boredapeyachtclubLinks to an external site. and select all apes with “Solid gold” fur and sort them “Price high to low” .  Use the URL for the subsequent coding.

(2)  Using Python or Java, write code that uses Selenium to access the URL from (1), click on each of the top-8 most expensive Bored Apes, and store the resulting details page to disk, “bayc_[N].htm” (replace [N] with the ape number).

 

MongoDB

(3)  Write code that goes through all 8 htm files downloaded in (2) and stores each ape’s name (its number) and all its attributes in a document inside a MongoDB collection called “bayc”.

 

Regular Webscraping

(4)  Yellow Pages uses GET requests for its search.  Using plain Python or Java (no Selenium), write a program that searches on yellowpages.com for the top 30 “Pizzeria” in San Francisco (no need to verify that the shop is actually selling pizzas, just search for “Pizzeria”, top 30 shops according to YP's "Default" sorting).  Save the search result page to disk, “sf_pizzeria_search_page.htm”.

 

(5)  Using Python or Java, write code that opens the search result page saved in (4) and parses out all shop information (search rank, name, linked URL [this store’s YP URL], star rating If It Exists, number of reviews IIE, TripAdvisor rating IIE, number of TA reviews IIE, “$” signs IIE, years in business IIE, review IIE, and amenities IIE).  Please be sure to skip all “Ad” results.

image.png

 

MongoDB

(6)  Copy your code from (5).  Modify the code to create a MongoDB collection called “sf_pizzerias” that stores all the extracted shop information, one document for each shop.

 

Parsing

(7)  Write code that reads all URLs stored in “sf_pizzerias” and download each shop page.  Store the page to disk, “sf_pizzerias_[SR].htm” (replace [SR] with the search rank).

 

(8)  Write code that reads the 30 shop pages saved in (7) and parses each shop’s address, phone number, and website.

 

API

(9)  Sign up for a free account with https://positionstack.com/Links to an external site.  Copy your code from (8).  Modify the code to query each shop address’ geolocation (long, lat).  Update each shop document on the MongoDB collection “sf_pizzerias” to contain the shop’s address, phone number, website, and geolocation.
